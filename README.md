# Deep-Learning-With-Large-Data-Set

We have created a CNN model by using FastAi library 

# about the model :


What the model does is, on providing a dataset, in the form of images, it
recognizes the patterns and similar properties in the whole dataset, thus accompanying
itself to this kind of problem and predicting the outcome. The next time you provide it
an image of the same kind but a different one, it will recognize the desired output and
give result based on accuracy.

So, hereâ€™s what I was given to appear a solution with-There are more than a million
species of flowers present on this planet and the task is to recognize some of them. You
are given a dataset of 102 different categories of flowers having nearly 20,000 images
of the same, you have to come up with a model which would predict the name of the
flower upon passing a new image to the model.

The language platform was python and the process which was to be undergone was
CNN (Convolutional Neural Network). So, there are 2 ways
in which you can implement CNN, one is, you create your own neural network which
will consist of layers to recognize different patterns(might have to compromise with
accuracy), or the other way round, using pre-trained models such as fastai, Vgg,
GoogleNet, Inception etc(accuracy is not an issue).So I did proceed with both the
approaches but found the accuracy around 99% in the latter case.

So, the thing is that once you get acquainted with these kinds of problems, it is quite
easy for one to explore beyond the given problem statement.

This experimentation can be further used to classify a different set of images having a
larger dataset or capturing live images from video and classifying them at runtime.
